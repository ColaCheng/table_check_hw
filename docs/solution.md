# Solution

This solution is according to my understanding of the assignment to implement. There are some parts of the angle we can discuss further and might change the implementation.

For example:
- The incoming data rate for this service
- The average size for each incoming data
- The API request rate for clients
- Considering the scale of data, we might need to compare the tradeoff between Relational Database and NoSQL database
- And so on

## Database

I choose PostgreSQL to store the order data because we need to aggregate the data to fit our situation.
The table schema and SQL query for each situation are as follows.

### Table schema

```sql
CREATE TABLE orders (
    id              bigserial,
    restaurant_name varchar(255),
    food_name       varchar(255),
    first_name      varchar(255),
    food_cost       float8,
    inserted_at     timestamp(0),
    updated_at      timestamp(0)
);

CREATE UNIQUE INDEX orders_restaurant_name_index ON orders (restaurant_name);
```

Currently I only have one `orders` table. But we can normalize the data to `restaurants`, `customers`, and `restaurant_customers` tables.

#### Further thought for database normalization

##### restaurants table

```sql
CREATE TABLE restaurants (
    id              uuid DEFAULT gen_random_uuid(),
    restaurant_name varchar(255) NOT NULL,
    inserted_at     timestamp(0),
    updated_at      timestamp(0)
);

CREATE UNIQUE INDEX restaurants_restaurant_name_index ON restaurants (restaurant_name);
```

##### customers table

```sql
CREATE TABLE customers (
    id              uuid DEFAULT gen_random_uuid(),
    first_name      varchar(255) NOT NULL,
    inserted_at     timestamp(0),
    updated_at      timestamp(0)
);

CREATE UNIQUE INDEX customers_first_name_index ON customers (first_name);
```

##### restaurant_customers table

```sql
CREATE TABLE restaurant_customers (
    id              uuid DEFAULT gen_random_uuid(),
    restaurant_id   uuid,
    customer_id     uuid,
    inserted_at     timestamp(0),
    updated_at      timestamp(0)
);
```

##### new orders table

```sql
CREATE TABLE orders (
    id                     bigserial,
    restaurant_name        varchar(255),
    food_name              varchar(255),
    food_cost              float8,
    restaurant_customer_id uuid,
    inserted_at            timestamp(0),
    updated_at             timestamp(0),
    CHECK (food_cost > 0)
);

CREATE UNIQUE INDEX orders_restaurant_customer_id_index ON orders (restaurant_customer_id);
```

### Query

The query was generated by `Ecto.Adapters.SQL.to_sql/3`.

* How many customers visited the "Restaurant at the end of the universe"?
```sql
SELECT count(o0.first_name) FROM orders AS o0 WHERE (o0.restaurant_name = $1);
```

> Note: `$1 = "the-restaurant-at-the-end-of-the-universe"`

* How much money did the "Restaurant at the end of the universe" make?
```sql
SELECT sum(o0.food_cost) FROM orders AS o0 WHERE (o0.restaurant_name = $1);
```

> Note: `$1 = "the-restaurant-at-the-end-of-the-universe"`

* What was the most popular dish at each restaurant?
```sql
SELECT s0.restaurant_name, s0.food_name, s0.count
FROM (
    SELECT
      ss0.restaurant_name AS restaurant_name,
      ss0.food_name AS food_name,
      ss0.count AS count,
      row_number() OVER (PARTITION BY ss0.restaurant_name ORDER BY ss0.count DESC) AS rank
    FROM (
        SELECT sso0.restaurant_name AS restaurant_name, sso0.food_name AS food_name, count(sso0.food_name) AS count
        FROM orders AS sso0
        GROUP BY sso0.restaurant_name, sso0.food_name
    ) AS ss0
) AS s0
WHERE (s0.rank = 1)
ORDER BY s0.count DESC;
```

> Note: Use `ROW_NUMBER()` function to assign a sequential integer number to each row that is partitioned by restaurant_name in the subquery’s result set and get the first one.

* What was the most profitable dish at each restaurant?
```sql
SELECT s0.restaurant_name, s0.food_name, s0.sum
FROM (
    SELECT
      ss0.restaurant_name AS restaurant_name,
      ss0.food_name AS food_name,
      ss0.sum AS sum,
      row_number() OVER (PARTITION BY ss0.restaurant_name ORDER BY ss0.sum DESC) AS rank
    FROM (
        SELECT sso0.restaurant_name AS restaurant_name, sso0.food_name AS food_name, sum(sso0.food_cost) AS sum
        FROM orders AS sso0
        GROUP BY sso0.restaurant_name, sso0.food_name
    ) AS ss0
) AS s0
WHERE (s0.rank = 1)
ORDER BY s0.sum DESC;
```

> Note: Use `ROW_NUMBER()` function to assign a sequential integer number to each row that is partitioned by restaurant_name in the subquery’s result set and get the first one.

* Who visited each store the most?
```sql
SELECT s0.restaurant_name, s0.first_name, s0.visit_count
FROM (
    SELECT
      ss0.restaurant_name AS restaurant_name,
      ss0.first_name AS first_name,
      ss0.visit_count AS visit_count,
      row_number() OVER (PARTITION BY ss0.restaurant_name ORDER BY ss0.restaurant_name DESC, ss0.visit_count DESC) AS rank
    FROM (
        SELECT sso0.restaurant_name AS restaurant_name, sso0.first_name AS first_name, count(sso0.first_name) AS visit_count
        FROM orders AS sso0
        GROUP BY sso0.restaurant_name, sso0.first_name
    ) AS ss0
) AS s0
WHERE (s0.rank = 1)
ORDER BY s0.visit_count DESC;
```

> Note: Use `ROW_NUMBER()` function to assign a sequential integer number to each row that is partitioned by restaurant_name in the subquery’s result set and get the first one.

* Who visited the most stores?
```sql
SELECT s0.first_name, s0.visit_count
FROM (
    SELECT
      ss0.first_name AS first_name,
      ss0.visit_count AS visit_count,
      row_number() OVER (ORDER BY ss0.visit_count DESC) AS rank
    FROM (
        SELECT sso0.first_name AS first_name, count(sso0.first_name) AS visit_count
        FROM orders AS sso0
        GROUP BY sso0.first_name
    ) AS ss0
) AS s0
WHERE (s0.rank = 1)
ORDER BY s0.visit_count DESC;
```

> Note: Use `ROW_NUMBER()` function to assign a sequential integer number to each row in the subquery’s result set and get the first one.

## Data ingestion

Use elixir script to parse the `data.csv` file and combine it with mix task(`ecto.setup`) to generate seeds data for the service.

> Note: We can move out seeds generation from `ecto.setup` if we want to separate this step. Then we need to manually run this `priv/repo/seeds.exs` script.

Run seeds script:
```
mix run priv/repo/seeds.exs
```

## HTTP server

Use `plug_cowboy` as HTTP server because I think using Phoenix to build a simple API server is a little bit overkill.
But I didn't against using Phoenix framework. Phoenix is great!

### Folder structure


```
├── lib
│   ├── routes
│   │   ├── order
|   |   |   ├── restaurant
|   |   |   |   ├── customers_count.ex
|   |   |   |   ├── revenue.ex
|   |   |   |   ├── top_customer.ex
|   |   |   |   ├── top_dish.ex
|   |   |   |   ├── top_profitable_dish.ex
|   |   |   └── top_customer.ex
|   |   ├── order.ex
|   |   └── public.ex
|   └── router.ex
└── *
```

- The `router.ex` is the entry point for all the endpoints.
- The `public.ex` is the entry point for public endpoints.
- The `order.ex` is the entry point for order API endpoints.
  - Break down the order API into `./routes/order` folder and implement each API handler logic.

## FAQs

### How would you build this differently if the data was being streamed from Kafka?

It should have a process pool to consume the data from Kafka and add the Kafka dependencies(like: `broadway_kafka`). The data processing should be in batch and have a retry mechanism if something goes wrong. After processing the data we should also batch insert the data to reduce the database overhead.

We might hit the data that is processed multiple times when the system is in a weird situation. We should figure out a way to avoid duplicate data. For example, make a unique id for each data.

From the consumer perspective, we should know the data is time critical or not. If yes, we need to make the processing quick enough to deliver to clients. Then we should keep monitoring the unconsumed message count in the topic not to pass a certain number.

### How would you improve the deployment of this system?

I already provide the `Dockerfile` so it can be easy to deploy the service into Kubernetes or other containerized environments. It will help to horizontal Scaling the service.

#### Things to set up

- CI/CD pipeline
  - For example: GitHub action to run the tests and build/deploy the docker image
- Metrics dashboard to monitor service
  - For example: Use Grafana/Datadog to make a dashboard and set up alerts if something goes wrong
- Code review process
- Simplify infrastructure provisioning
  - For example: Terraform, Ansible, Argo, and so on that depending on the infrastructure.
